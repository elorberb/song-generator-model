{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elorberb/song-generator-model/blob/main/deep3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_pF2niSAZQE",
        "outputId": "e643a372-bf2d-480e-f8ae-9737526dabfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pretty_midi in /usr/local/lib/python3.8/dist-packages (0.2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from pretty_midi) (1.15.0)\n",
            "Requirement already satisfied: mido>=1.1.16 in /usr/local/lib/python3.8/dist-packages (from pretty_midi) (1.2.10)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from pretty_midi) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.8/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pytorch-nlp) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from pytorch-nlp) (4.64.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.8/dist-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pretty_midi\n",
        "!pip install pytorch-nlp\n",
        "!pip install torchsummary"
      ],
      "id": "3_pF2niSAZQE"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "134e25ad",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from functools import reduce\n",
        "import re\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.distributions import Categorical\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pretty_midi\n",
        "\n",
        "import gensim.downloader\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from torch.distributions.one_hot_categorical import OneHotCategorical\n"
      ],
      "id": "134e25ad"
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "wswa22MdYmiH"
      },
      "id": "wswa22MdYmiH",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiVcRx6YPFG5",
        "outputId": "83a8e68f-480b-4654-d55f-e62ae2f8007f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "BiVcRx6YPFG5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78712570"
      },
      "source": [
        "First we'll load the csv dataset which contains the lyrics and other information about the songs\n",
        "since the dataset doesn't have a header column we'll use `header=None` and only load the first 3 columns with `usecols=[1,2,3]`\n",
        "\n",
        "Then we'll rename the columns to something more readable"
      ],
      "id": "78712570"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "85b40bc8"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/lyrics_train_set.csv', header=None, usecols=[0,1,2])\n",
        "df.columns = ['Artist','Name', 'Lyrics']"
      ],
      "id": "85b40bc8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61bca9ac"
      },
      "source": [
        "This code will create our vocabulary of possible tokens we can predict"
      ],
      "id": "61bca9ac"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54acb6a2"
      },
      "source": [
        "now we'll load a pre-train word embedding model, to load models to gensim we can use the gensim downloader.\n",
        "\n",
        "A list of pre-trained models can be found [here](https://github.com/RaRe-Technologies/gensim-data)"
      ],
      "id": "54acb6a2"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cf3cf697",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8adeb6e4-7551-469d-fb61-d11af9447864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "w2v = gensim.downloader.load('word2vec-google-news-300')"
      ],
      "id": "cf3cf697"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dfdf13a"
      },
      "source": [
        "now we'll build our dataset, in this version we'll build a simple dataset that doesn't require padding or truncation of the lyrics. This has the advantange of being able to use the whole lyrics for training, but the downside is that it'll force us to use a batch size of 1."
      ],
      "id": "6dfdf13a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Helper Functions"
      ],
      "metadata": {
        "id": "N8OyRsMYGe3p"
      },
      "id": "N8OyRsMYGe3p"
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text)\n",
        "  # Replace '&' with 'newLine'\n",
        "  text = text.replace(\"&\", \"newLine\")\n",
        "  # Remove brackets and their contents from the lyrics\n",
        "  text = re.sub(r\"\\[.*\\]\", \"\", text)\n",
        "\n",
        "  replacements = {\n",
        "      r\"\\'ve\": ' have',\n",
        "      r\"\\'ll\": ' will',\n",
        "      r\"won't\": 'will not',\n",
        "      r\"\\bwon't\\b\": 'will not',\n",
        "      r\"i'm\": 'i am',\n",
        "      r\"he's\": 'he is',\n",
        "      r\"she's\": 'she is',\n",
        "      r\"it's\": 'it is',\n",
        "      r\"we're\": 'we are',\n",
        "      r\"you're\": 'you are',\n",
        "      r\"they're\": 'they are',\n",
        "      r\"who'se\": 'who is',\n",
        "      r\"who're\": 'who are',\n",
        "      r\"what's\": 'what is',\n",
        "      r\"where's\": 'where is',\n",
        "      r\"y'all\": 'you all',\n",
        "      r\"\\'d\": ' would',\n",
        "      r\"ain't\": 'are not',\n",
        "      r\"can't\": 'can not',\n",
        "      r\"evry\": 'every',\n",
        "      r\"n't\": 'not',\n",
        "      r\"\\'s\": '',\n",
        "      r\"\\'\": '',\n",
        "      r\"hasnot\": 'has not',\n",
        "      r\"doesnot\": 'does not',\n",
        "      r\"dont\": 'do not',\n",
        "      r\"doesnt\": 'does not',\n",
        "      r\"didnt\": 'did not',\n",
        "      r\"hasnt\": 'has not',\n",
        "      r\"aint\": 'is not',\n",
        "      r\"im\": 'i am',\n",
        "      r\"youre\": 'you are',\n",
        "      r\"youve\": 'you have',\n",
        "      r\"\\b[Uu]s\\b\": 'we',\n",
        "      r\"\\bthats\\b\": 'that is',\n",
        "      r\"\\bwerent\\b\": 'were not',\n",
        "      r\"couldnot\": 'could not',\n",
        "      r\"wouldnot\": 'would not',\n",
        "      r\"isnot\": 'is not',\n",
        "      r\"havenot\": 'have not',\n",
        "      r\"shouldnot\": 'have not',\n",
        "      r\"donot\": 'do not',\n",
        "      r\"arenot\": 'are not',\n",
        "      r\"wasnot\": 'was not'\n",
        "  }\n",
        "  for pattern, replacement in replacements.items():\n",
        "      text = re.sub(pattern, replacement, text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "5-wbJngJGipG"
      },
      "id": "5-wbJngJGipG",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MIDI data extractor Functions"
      ],
      "metadata": {
        "id": "n-Hza-3ACYRI"
      },
      "id": "n-Hza-3ACYRI"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_avg_note_pitch(pm):\n",
        "    \"\"\"\n",
        "    Calculate the average pitch of the notes in the instrument with the most notes in a MIDI file.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pm : pretty_midi.PrettyMIDI\n",
        "        The MIDI file.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    avg_pitch : float\n",
        "        The average pitch of the notes in the instrument with the most notes.\n",
        "    \"\"\"\n",
        "    max_notes_instrument = pm.instruments[0]\n",
        "    for instrument in pm.instruments:\n",
        "        if len(instrument.notes) > len(max_notes_instrument.notes):\n",
        "            max_notes_instrument = instrument\n",
        "\n",
        "    # Extract the pitches of the notes in the max notes instrument\n",
        "    pitches = [note.pitch for note in max_notes_instrument.notes]\n",
        "    avg_pitch = np.average(pitches)\n",
        "\n",
        "    return avg_pitch\n",
        "\n",
        "\n",
        "def get_instruments_list(pm):\n",
        "    \"\"\"\n",
        "    Get a list of the instruments present in a MIDI file.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pm : pretty_midi.PrettyMIDI\n",
        "        The MIDI file.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    instruments_list : numpy array of shape (128,)\n",
        "        An array where the i-th element is 1 if the i-th instrument is present in the MIDI file, and 0 otherwise.\n",
        "    \"\"\"\n",
        "    # Initialize an array to store the presence of each instrument\n",
        "    instruments_list = np.zeros(128)\n",
        "    for instrument in pm.instruments:\n",
        "        # Try to convert the instrument name to a program number\n",
        "        try:\n",
        "            instrument_program = pretty_midi.instrument_name_to_program(instrument.name)\n",
        "            # Mark the instrument as present in the list\n",
        "            instruments_list[instrument_program] = 1\n",
        "        except Exception as e_ins:\n",
        "            pass\n",
        "\n",
        "    return instruments_list\n",
        "\n",
        "def get_notes_statistics(pm):\n",
        "    \"\"\"\n",
        "  Calculate statistics on the number of notes in a MIDI file.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  pm : pretty_midi.PrettyMIDI\n",
        "      The MIDI file.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  sum_notes : int\n",
        "      The total number of notes in the MIDI file.\n",
        "  average_notes : float\n",
        "      The average number of notes per instrument in the MIDI file.\n",
        "  min_notes : int\n",
        "      The minimum number of notes in any instrument in the MIDI file.\n",
        "  max_notes : int\n",
        "      The maximum number of notes in any instrument in the MIDI file.\n",
        "  \"\"\"\n",
        "    sum_notes = 0\n",
        "    min_notes, max_notes = float(\"inf\"), float(\"-inf\")\n",
        "\n",
        "    for instrument in pm.instruments:\n",
        "        num_notes = len(instrument.notes)\n",
        "\n",
        "        # Update the sum, min, and max\n",
        "        sum_notes += num_notes\n",
        "        min_notes = min(min_notes, num_notes)\n",
        "        max_notes = max(max_notes, num_notes)\n",
        "\n",
        "    # Calculate the average number of notes\n",
        "    average_notes = sum_notes / len(pm.instruments)\n",
        "\n",
        "    return sum_notes, average_notes, max_notes, min_notes\n",
        "\n",
        "\n",
        "def normalize_feature_vector(vector: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Normalize a feature vector.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    vector : numpy array\n",
        "        The input feature vector.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    vector : numpy array\n",
        "        The input feature vector, normalized.\n",
        "    \"\"\"\n",
        "    scaler = MinMaxScaler()\n",
        "    vector = scaler.fit_transform(vector.reshape(-1, 1)).flatten()\n",
        "    return vector"
      ],
      "metadata": {
        "id": "5vWtCE4dCcjx"
      },
      "id": "5vWtCE4dCcjx",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "881c14a9"
      },
      "outputs": [],
      "source": [
        "class LyricsDataset(Dataset):\n",
        "    def __init__(self, df, w2v_model, vocab, midi_path):\n",
        "        # Our dataset will receive 4 paramters\n",
        "        # the dataframe with the lyrics and metadata\n",
        "        # the pre-trained word2vec model\n",
        "        # and path to where all the midi files sit\n",
        "        # vocabulary of all the possible words we can have\n",
        "        self.df = df\n",
        "        self.w2v_model = w2v_model\n",
        "        self.midi_path = midi_path\n",
        "        self.vocab = vocab\n",
        "        \n",
        "        # if you desire to preprocess the lyrics, clean the text and more\n",
        "        # you can change the preprocess lyrics method to do cleaning as you desire\n",
        "        # right now the method is doing the identity function\n",
        "        # which mean the lyrics aren't getting changed at all.\n",
        "        self.df['Lyrics'] = self.df['Lyrics'].apply(self.preprocess_lyrics)\n",
        "        \n",
        "        \n",
        "        # we'll now create our mapping from words to indexes\n",
        "        # this is done so we can later convert the predictions of the model to actual words\n",
        "        # and for the training to convert the words into labels the model can use for back propagation\n",
        "        self.w2i = {w: i for i, w in enumerate(self.vocab)}\n",
        "        self.i2w = {i: w for w, i in self.w2i.items()}\n",
        "        self.i2e = {}\n",
        "\n",
        "\n",
        "    def preprocess_lyrics(self, lyrics):\n",
        "        # change this function as you desire\n",
        "        lyrics = clean_text(lyrics)\n",
        "        return lyrics\n",
        "\n",
        "    def __len__(self, ):\n",
        "        return self.df.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # in the get item method we'll do something simple\n",
        "        # for each song we'll convert the entire song to word vectors\n",
        "        # and generate 1 midi feature vector that we'll concat to all the word vectors\n",
        "        \n",
        "        artist, song_name, lyrics = df.iloc[idx]\n",
        "        \n",
        "        # get the midi features\n",
        "        midi_features = self.get_midi_features(artist, song_name)\n",
        "        \n",
        "        # get the word vectors and corresponding labels\n",
        "        word_vectors, labels = self.get_word_vectors(lyrics) \n",
        "        \n",
        "        # repeats the same vector to match the shape for the song word vectors\n",
        "        # if for example we had 350 words in our songs\n",
        "        # word vectros is shape (350, 300)\n",
        "        # this will make midi_features shape (350, 141)\n",
        "        midi_features = midi_features.repeat(word_vectors.size(0), 1)\n",
        "        \n",
        "        # now we contact the both input to make the input that the model will receive\n",
        "        # which is shape (num words in song, 441)\n",
        "        inputs = torch.cat([word_vectors, midi_features], dim=-1)\n",
        "\n",
        "        #update the dict from tokens to embeddings\n",
        "        self.update_i2e(inputs, labels)\n",
        "        \n",
        "        return inputs, labels\n",
        "\n",
        "    def update_i2e(self, inputs, labels):\n",
        "        '''\n",
        "        This function updates the dictionary self.d with the input embeddings and labels.\n",
        "        The keys of the dictionary are the labels (tokens) and the values are the corresponding input embeddings.\n",
        "\n",
        "        Parameters:\n",
        "            inputs (Tensor): A tensor of input embeddings.\n",
        "            labels (Tensor): A tensor of labels (tokens).\n",
        "        '''\n",
        "        # Zip the inputs and labels into a list of tuples\n",
        "        pairs = zip(inputs, labels)\n",
        "        # Iterate through the pairs and update the dictionary\n",
        "        for input, label in pairs:\n",
        "          label = label.item()\n",
        "          self.i2e[label] = input\n",
        "        \n",
        "    def get_midi_features(self, artist, song_name):\n",
        "        # this method creates a single midi feature vector for the entire song\n",
        "        midi_file = self.get_midi_file(artist, song_name)\n",
        "        \n",
        "        try: \n",
        "            # load the midi file\n",
        "            midi = pretty_midi.PrettyMIDI(os.path.join(self.midi_path, midi_file))\n",
        "            \n",
        "            number_instruments = len(midi.instruments)\n",
        "            # extracting the length of time signature changes\n",
        "            tsc = len(midi.time_signature_changes)\n",
        "            # extracting the highest probability tempo estimation\n",
        "            best_tempo = midi.estimate_tempo()\n",
        "            # extracting the number of notes per instrument (sum, average, min, max)\n",
        "            sum_notes, average_notes, max_notes, min_notes = get_notes_statistics(midi)\n",
        "            # to add the average noe pitch\n",
        "            avg_note_pitch = get_avg_note_pitch(midi)\n",
        "            # extracting which instruments participate in the midi\n",
        "            instruments_list = get_instruments_list(midi)\n",
        "            # get piano roll\n",
        "            piano_roll = midi.get_piano_roll().mean(-1)\n",
        "\n",
        "            # the feature vector create is shape 147\n",
        "            # is shape (1,)\n",
        "            midi_features = np.concatenate((np.array([tsc, sum_notes, average_notes, max_notes, min_notes, avg_note_pitch, piano_roll]),instruments_list, [piano_roll]))\n",
        "            midi_features_norm = normalize_feature_vector(midi_features)\n",
        "            midi_features_norm = torch.from_numpy(midi_features_norm).float()\n",
        "        except Exception as e:\n",
        "            # if for some odd reason we can't load the midi file\n",
        "            # or there are problem with the feature extraction\n",
        "            # we'll just create a vector of 0's instead\n",
        "            midi_features_norm = torch.zeros((147,), dtype=torch.float32)\n",
        "\n",
        "            \n",
        "        return midi_features_norm\n",
        "    \n",
        "    def get_word_vectors(self, lyrics):\n",
        "        # this method iterates over all the words in the song\n",
        "        # for each for it try to take the word vector from the word2vec model\n",
        "        # if the word2vec model's vocabulary doesn't contains a word in the song\n",
        "        # it'll instead create a feature vectors with the same size of the word embedding\n",
        "        vectors = []\n",
        "        labels = []\n",
        "                \n",
        "        for word in lyrics.split(' '):\n",
        "            # associate the correct label for that vector\n",
        "            labels.append(self.w2i[word])\n",
        "            # checks if the word exists in the w2v model\n",
        "            if word in self.w2v_model:\n",
        "                # takes the word embedding for that vector\n",
        "                vectors.append(self.w2v_model[word])\n",
        "            else:\n",
        "                # if not create a vector of zeros\n",
        "                vectors.append(np.zeros((300,)))\n",
        "        \n",
        "        # create the torch tensor shape (num words, 300)\n",
        "        vectors = torch.from_numpy(np.stack(vectors)).float()\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "       \n",
        "        return vectors, labels\n",
        "\n",
        "   \n",
        "    def get_midi_file(self, artist, song_name):\n",
        "        # since the midi file names are cased and artist and song name are lower cased only\n",
        "        # this method finds the midi file on the disk that correspond to the current artist and song name\n",
        "        # we want to process into inputs for the model\n",
        "        artist = '_'.join(artist.split(' '))\n",
        "        song_name = '_'.join(song_name.split(' '))\n",
        "        file_name = artist + '_-_' + song_name + '.mid'\n",
        "        \n",
        "        files = os.listdir(self.midi_path)\n",
        "        midi_file = next(filter(lambda x: x.lower() == file_name, files))\n",
        "        \n",
        "        return midi_file"
      ],
      "id": "881c14a9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CV0fX0T1edI"
      },
      "source": [
        "###Building the RNN Model"
      ],
      "id": "4CV0fX0T1edI"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ecad474b"
      },
      "outputs": [],
      "source": [
        "class LyricsModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, vocab_size, num_layers=1, \n",
        "                 dropout=0.3):\n",
        "        super(LyricsModel, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.lstm = nn.GRU(input_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def forward(self, x, h=None, return_state=True):\n",
        "        \n",
        "        if h is None:\n",
        "          h = self.init_hidden(x.size(0))\n",
        "\n",
        "        out, h = self.lstm(x, h)\n",
        "        out = self.dropout(out)\n",
        "        logits = self.linear(out)\n",
        "        \n",
        "        if return_state:\n",
        "            return logits, h\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "    def init_hidden(self, batch_size, device='cuda'):\n",
        "        return torch.zeros((self.num_layers, batch_size, self.hidden_size), device=device)"
      ],
      "id": "ecad474b"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJBe-Akehncc",
        "outputId": "366a6f56-20e0-4753-a293-e1bf4339204a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "id": "ZJBe-Akehncc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Elq7-ze5csSg"
      },
      "source": [
        "####Define Vocablulary, Datasets and Dataloaders"
      ],
      "id": "Elq7-ze5csSg"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "8c10fa18"
      },
      "outputs": [],
      "source": [
        "vocab = set()\n",
        "for lyrics in df.Lyrics.tolist():\n",
        "    lyrics = lyrics.split(' ')\n",
        "    vocab |= set(lyrics)"
      ],
      "id": "8c10fa18"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "5d883aa8"
      },
      "outputs": [],
      "source": [
        "train_df, val_df = train_test_split(df, test_size=0.1, shuffle=True)"
      ],
      "id": "5d883aa8"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "eb66edc1"
      },
      "outputs": [],
      "source": [
        "midi_path = '/content/drive/MyDrive/midi_files/' # change to your midi path\n",
        "\n",
        "train_dataset = LyricsDataset(train_df, w2v, vocab, midi_path)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "val_dataset = LyricsDataset(val_df, w2v, vocab, midi_path)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)"
      ],
      "id": "eb66edc1"
    },
    {
      "cell_type": "code",
      "source": [
        "for i, data in enumerate(train_dataloader):\n",
        "    # get the inputs and labels\n",
        "    inputs, labels = data"
      ],
      "metadata": {
        "id": "Znp_oht7787d"
      },
      "id": "Znp_oht7787d",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.i2e[1322].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya2_Wgkl8PNJ",
        "outputId": "75fe7f44-19ec-4465-8511-008f9187f100"
      },
      "id": "Ya2_Wgkl8PNJ",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([447])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eNxWL_acpMS"
      },
      "source": [
        "###Define Model"
      ],
      "id": "2eNxWL_acpMS"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mfL9mfiCKxj1"
      },
      "outputs": [],
      "source": [
        "#get the size of the embeddings\n",
        "embedding_size = train_dataset[0][0].shape[1]\n",
        "model = LyricsModel(input_size=embedding_size,  hidden_size=64, vocab_size=len(vocab)).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "id": "mfL9mfiCKxj1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF0JTvFHcedt"
      },
      "source": [
        "###Train the model"
      ],
      "id": "gF0JTvFHcedt"
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, criterion, optimizer, train_dataloader, device, epoches):\n",
        "    '''\n",
        "    This function trains a model on a given dataset.\n",
        "\n",
        "    Parameters:\n",
        "        model (nn.Module): The model to be trained.\n",
        "        criterion (nn.Module): The loss function to use during training.\n",
        "        optimizer (optim.Optimizer): The optimizer to use during training.\n",
        "        train_dataloader (DataLoader): The dataloader for the training dataset.\n",
        "        epoches (integer): Number of epoches we run.\n",
        "        device (torch.device): The device on which to run the model.\n",
        "\n",
        "    Returns:\n",
        "        model (nn.Module): The trained model.\n",
        "    '''\n",
        "    model.train()\n",
        "    for epoch in range(epoches):\n",
        "        for step, batch in enumerate(train_dataloader):  \n",
        "            x = batch[0].to(device)  \n",
        "            y = batch[1].to(device)\n",
        "\n",
        "            preds = model(x, None, return_state=False)\n",
        "            loss = criterion(preds.transpose(-1, -2), y)  # Calculate the loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()  # Backpropagate the loss\n",
        "            optimizer.step()  # Update the model parameters\n",
        "\n",
        "        print(f'Epoch: {epoch}, Loss: {loss}')  # Print the loss for each epoch\n",
        "\n",
        "    return model  # Return the trained model"
      ],
      "metadata": {
        "id": "j83vSJ5jkXsq"
      },
      "id": "j83vSJ5jkXsq",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train(model, criterion, optimizer, train_dataloader, device, epoches=20)"
      ],
      "metadata": {
        "id": "Pcz_h_Aakxlc"
      },
      "id": "Pcz_h_Aakxlc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_w2e(word, dataset):\n",
        "  '''\n",
        "  This function searches for the embedding value of a given word in a dataset.\n",
        "  If the word is not in the dataset, it returns an embedding of all zeros.\n",
        "\n",
        "  Parameters:\n",
        "    word (str): The word to search for.\n",
        "    dataset (Dataset): The dataset to search.\n",
        "\n",
        "  Returns:\n",
        "    embedding (Tensor): The embedding value of the word token.\n",
        "  '''\n",
        "  # Check if the word is in the dataset's Word2Vec model\n",
        "  if word in dataset.w2v_model:\n",
        "      # Get the embedding for the word\n",
        "      embedding = dataset.w2v_model[word]\n",
        "  else:\n",
        "      # If the word is not in the model, return a zero embedding\n",
        "      embedding = np.zeros(300,)\n",
        "\n",
        "  # Convert the embedding to a PyTorch Tensor and concatenate it with the midi features tensor\n",
        "  embedding = torch.tensor(embedding).float()\n",
        "  midi_features = torch.zeros((147,), dtype=torch.float32)\n",
        "  next_word = torch.cat([embedding, midi_features], dim=-1)\n",
        "\n",
        "  return next_word"
      ],
      "metadata": {
        "id": "bjYSoRD6nlBj"
      },
      "id": "bjYSoRD6nlBj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "S-tDfR-xMPdC"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def predict(dataset, model, song, device, num_words=20):\n",
        "    '''\n",
        "    This function predicts the next words in a song given a model and a starting point.\n",
        "\n",
        "    Parameters:\n",
        "        dataset (Dataset): The dataset object containing the mapping of word indices to words.\n",
        "        model (nn.Module): The trained model used to make predictions.\n",
        "        song (Tensor): A tensor representing the starting point of the song.\n",
        "        device (torch.device): The device on which to run the model.\n",
        "        num_words (int, optional): The number of words to predict. Default is 20.\n",
        "\n",
        "    Returns:\n",
        "        words (list): A list of the predicted words.\n",
        "    '''\n",
        "    def preprocess_word(word, device):\n",
        "      \"\"\"Change word vector to the required shape (1,1,447)\"\"\"\n",
        "      return word.unsqueeze(0).unsqueeze(1).to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    words = []\n",
        "    state_h = model.init_hidden(1).to(device)\n",
        "    word = song[0][0][0]\n",
        "    for i in range(0, num_words):\n",
        "        x = preprocess_word(word, device)\n",
        "        y_pred, state_h = model(x, state_h)\n",
        "        dist = OneHotCategorical(logits=y_pred)\n",
        "        word_index = dist.sample()\n",
        "        token = word_index.argmax(-1).item()\n",
        "        word = dataset.i2w[token]\n",
        "        words.append(word)\n",
        "        try:\n",
        "          word = dataset.i2e[token]\n",
        "        except:\n",
        "          word = song[0][0][i]\n",
        "\n",
        "    return words"
      ],
      "id": "S-tDfR-xMPdC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRxGI_b02_pl"
      },
      "outputs": [],
      "source": [
        "def create_song(dataset, model, dataloader, device, num_words):\n",
        "  '''\n",
        "  This function creates a song using a trained model and a starting point.\n",
        "\n",
        "  Parameters:\n",
        "      dataset (Dataset): The dataset object containing the mapping of word indices to words.\n",
        "      model (nn.Module): The trained model used to make predictions.\n",
        "      dataloader (DataLoader): A dataloader containing the starting point for the song.\n",
        "      device (torch.device): The device on which to run the model.\n",
        "      num_words (int): The number of words to include in the song.\n",
        "\n",
        "  Returns:\n",
        "      song (str): A string containing the generated song.\n",
        "  '''\n",
        "  words = predict(dataset, model, dataloader, device, num_words=num_words)\n",
        "  song = ' '.join(words).replace('& ', '\\n')\n",
        "  return song"
      ],
      "id": "VRxGI_b02_pl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHn60c-4MH2y"
      },
      "outputs": [],
      "source": [
        "new_song = create_song(train_dataset, model, next(iter(val_dataloader)), device, num_words=100)"
      ],
      "id": "AHn60c-4MH2y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOwgfxmTbai1"
      },
      "outputs": [],
      "source": [
        "print(new_song)"
      ],
      "id": "WOwgfxmTbai1"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}